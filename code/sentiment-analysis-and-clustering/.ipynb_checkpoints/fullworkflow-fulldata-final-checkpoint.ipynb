{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Workflow\n",
    "\n",
    "Feature extraction + Word embedding (only pretrained ones) + Clustering + Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature Extraction\n",
    "\n",
    "Already done, use the csv output from that instead of running the code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 1.22MB/s]\n",
      "2021-03-26 01:21:26 INFO: Downloading default packages for language: en (English)...\n",
      "2021-03-26 01:21:27 INFO: File exists: C:\\Users\\TzeMin\\stanza_resources\\en\\default.zip.\n",
      "2021-03-26 01:21:35 INFO: Finished downloading models and saved to C:\\Users\\TzeMin\\stanza_resources.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TzeMin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TzeMin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\TzeMin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import regex\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import stanza\n",
    "stanza.download('en') # download English model\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ippt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ipt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sessions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>still</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>vary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>mailbox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>forsee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>save</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>win</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1771 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index\n",
       "0         ippt\n",
       "1          ipt\n",
       "2     sessions\n",
       "3        still\n",
       "4           rt\n",
       "...        ...\n",
       "1766      vary\n",
       "1767   mailbox\n",
       "1768    forsee\n",
       "1769      save\n",
       "1770       win\n",
       "\n",
       "[1771 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refined = pd.read_csv(\"../../output/corpus-refined-features.csv\", usecols = ['index'])\n",
    "refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           ippt\n",
       "1            ipt\n",
       "2       sessions\n",
       "3          still\n",
       "4             rt\n",
       "          ...   \n",
       "1766        vary\n",
       "1767     mailbox\n",
       "1768      forsee\n",
       "1769        save\n",
       "1770         win\n",
       "Name: index, Length: 1771, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncleaned_words = refined['index']\n",
    "uncleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words removed were:  {'serious', 'everything', 'everyone', 'move', 'wherein', 'used', 'others', 'amount', 'still', 'somewhere', 'nothing', 'full', 'much', 'see', 'mine', 'give', 'call', 'next', 'part', 'anyone', 'becomes', 'put', 'yet', 'anything', 'go', 'latter', 'say', 'first', 'seem', 'nobody', 'alone', 'something', 'side', 'show', 'whole', 'get', 'enough', 'take', 'never', 'name', 'last', 'many', 'one', 'someone', 'become', 'keep', 'make', 'back', 'ten', 'none', 'top', 'less'}\n",
      "From 1771 to 1719\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ippt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ipt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sessions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>window</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714</th>\n",
       "      <td>vary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>mailbox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>forsee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>save</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1718</th>\n",
       "      <td>win</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1719 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          word\n",
       "0         ippt\n",
       "1          ipt\n",
       "2     sessions\n",
       "3           rt\n",
       "4       window\n",
       "...        ...\n",
       "1714      vary\n",
       "1715   mailbox\n",
       "1716    forsee\n",
       "1717      save\n",
       "1718       win\n",
       "\n",
       "[1719 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # to run on command prompt: python -m spacy download en_core_web_sm\n",
    "words = [item for item in uncleaned_words if item not in nlp.Defaults.stop_words]\n",
    "\n",
    "print(\"Words removed were: \", set(uncleaned_words).difference(set(words)))\n",
    "print(\"From\", len(uncleaned_words), \"to\", len(words))\n",
    "\n",
    "words_df = pd.DataFrame(words)\n",
    "words_df.columns = ['word']\n",
    "words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word Embedding + Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 spaCy's Pretained Vectors + Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tzemin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3337: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "c:\\users\\tzemin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:154: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 1.0 (renaming of 0.25) which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "def vectorize(text):\n",
    "    \"\"\"Get the SpaCy vector corresponding to a text\"\"\"\n",
    "    return nlp(text).vector\n",
    "\n",
    "X = np.stack(vectorize(word) for word in words)\n",
    "X_normalised = normalize(np.stack(vectorize(word) for word in words))\n",
    "\n",
    "affprop = AffinityPropagation()\n",
    "affprop.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - *dont:* cant, didnt, doesnt, dont, havent, wont\n",
      " - *complete:* birdy, clean, clear, complete, correct, exempt, liable, open, qualify, select, sorry, strict, suitable\n",
      " - *long:* bully, early, far, hard, kinda, late, long, mcs, nsdotsg, overseas, rly, worst\n",
      " - *days:* days, hours, months, thanks, times, ways, weeks, years\n",
      " - *good:* advisable, bad, brief, compulsory, cool, curious, dependent, different, dusty, fine, funny, good, great, green, haiz, incamp, mandatory, maximum, meaningful, mindef, miserable, nice, present, proactive, public, regular, right, siong, stringent, tough, true, useless, vague, weird, wrong\n",
      " - *week:* april, bit, day, december, hour, july, june, month, night, november, time, way, week, weekend, year\n",
      " - *letter:* abuse, background, base, bill, blur, butt, concern, cookhouse, degree, direction, division, dun, experience, eye, factor, fi, force, incident, job, kana, kit, language, letter, mission, mob, moi, mustache, period, prep, privilege, programme, quiz, rate, rod, role, route, rush, session, siao, site, slot, solution, station, system, tip, wah, waiver, zone\n",
      " - *mr:* mr, sia, sir\n",
      " - *thats:* hdb, hows, im, ns, tahan, thats, theres, whats\n",
      " - *pay:* abit, amend, clock, cover, dec, drag, earn, enjoy, garang, implement, keng, knock, pay, ps, roll, send, shit, singpass, teach, win, write\n",
      " - *park:* bmtc, buay, chest, deployment, error, fee, grenade, heart, helpdesk, mailbox, op, park, press, proficiency, query, ranger, scenario, screen, script, setup, soldier, staff, stance, status, tcss, unit\n",
      " - *kind:* kind, sort\n",
      " - *questions:* arrangements, benefits, cases, chances, conditions, criteria, details, differences, instructions, items, keys, lots, opportunities, personnel, personnels, problems, questions, reasons, secs, signs, specs, standards, tags, things, tights, venues, versions, vocations, weekends, wheels, words\n",
      " - *report:* announce, answer, appeal, arrange, bookout, capture, cause, centre, change, claim, confuse, crowd, delay, doubt, dude, end, fear, fix, guarantee, help, hope, lesson, limit, matter, note, notice, pang, place, practice, range, recruit, report, result, review, sign, study, summary, tally, test, use, view\n",
      " - *information:* access, action, activity, alot, announcement, anxiety, attention, attitude, authority, blood, cancellation, completion, confidence, confusion, correction, diagnosis, difference, difficulty, exemption, exit, extension, failure, flexibility, food, information, initiative, interest, leadership, life, money, notification, patience, payment, peace, penalty, placement, power, proof, punishment, purpose, responsibility, section, shortage, speculation, trial, trip, trouble, weather\n",
      " - *advice:* activation, address, advice, allowance, application, case, chance, checkup, clarification, consideration, date, decision, effect, effort, fallout, goal, idea, incentive, indication, info, intensity, nssc, obligation, opinion, opportunity, option, order, participation, pattern, performance, permission, pressure, question, reason, source, statement, strength, suggestion, topic, vocation, word\n",
      " - *heard:* got, heard, meant, saw, thought, wanted\n",
      " - *cos:* cos\n",
      " - *saturday:* bday, everyday, friday, saturday, sunday, thursday, tomorrow, weekday\n",
      " - *big:* average, big, close, common, direct, electronic, familiar, high, important, impossible, knowledgeable, logical, low, meaningless, mental, new, normal, old, optional, portal, professional, rare, severe, similar, small, spare, subjective, temporary, uni, warm\n",
      " - *meet:* act, arrive, avoid, calander, clarify, compare, defer, eg, fight, focus, forsee, happen, increase, login, look, lose, manage, meet, participate, reap, register, reset, resume, return, rmb, sleep, solve, stand, stop, survive, verify, waive, walk, zzzz\n",
      " - *total:* airforce, bt, chao, hong, hsp, mon, ooc, outdoor, overstress, pax, round, screenshot, sian, spf, standard, toh, total, uniform, yea\n",
      " - *higher:* better, easier, higher, lesser, newer, older, smaller, tougher, worse\n",
      " - *conduct:* assist, attempt, block, catch, charge, conduct, drive, edit, enrollment, excuse, fault, gain, grant, greent, inform, luck, parking, pop, punish, pushup, regret, release, resort, retake, rule, shift, squeeze, update, visit, waste\n",
      " - *individual:* anal, black, blue, chi, english, fri, individual, inpro, key, manual, medical, mobile, official, sial, simple, special\n",
      " - *silver:* ass, beard, biz, carpark, chiong, cny, confinement, ext, frequency, fun, future, gold, journey, laptop, length, light, magic, navy, nsman, outfield, ptis, record, sergeant, silver, singapore\n",
      " - *commanders:* bands, brothers, clothes, commanders, concerns, degrees, documents, downpes, fingers, forces, friends, holders, liabilities, liddat, lights, lockers, males, members, missions, moderators, oots, options, police, protocols, semesters, servicemen, slippers, specialists, sports, studies, trainers\n",
      " - *sense:* course, example, fact, moment, point, sense\n",
      " - *run:* bah, beat, bro, come, cso, cut, hq, ite, jog, live, offset, pti, read, relax, rem, rotate, run, sat, scdf, screw, sept, shot, sit, specialise, spread, straighten, streamline, turn\n",
      " - *reservists:* actions, areas, arms, belongings, bookings, boots, bros, camps, casualties, choices, commitments, defaulters, facilities, grades, ideas, instructors, ippts, ipts, issues, jobs, machines, offences, ones, opinions, organisations, palms, phones, posts, programs, pushups, rates, requirements, reservists, resources, services, shoes, systems, takers, targets, toilets, tracks, trainees, trainings, types, zones\n",
      " - *confident:* able, afraid, akin, applicable, confident, free, happy, ready, sure, unable, unlucky, willing\n",
      " - *fb:* bo, co, ex, fb, fr, id, ok, sf, sgt\n",
      " - *apart:* alrdy, apart, away, forward, later\n",
      " - *support:* bug, circuit, comment, condition, contact, control, convince, default, demand, diploma, directive, encounter, entitlement, exchange, feed, form, guardhouse, guideline, harm, impact, maximise, plan, position, posture, request, situation, stress, support, transport, travel, trust, version, weakness\n",
      " - *ground:* app, asa, bedok, booklet, cham, chiongsua, cost, ground, hiit, hks, hta, ihl, ipt, lah, ppt, scs, suay\n",
      " - *yo:* tt, ya, yo\n",
      " - *complain:* advise, aim, apr, ask, assume, believe, cb, click, complain, determine, discuss, feel, forget, hearsay, know, leap, niang, pray, remain, require, suppose, tell, thank, think, wait, wonder, worry, wose\n",
      " - *noob:* dk, ft, noob, nov, req, saf, tho, yr\n",
      " - *cleared:* answered, asked, attended, aussie, booked, cancelled, checked, chose, cleared, counted, gotten, granted, hit, idti, launched, left, noted, posted, received, sent, set, shut, stated, told, tried, ulu\n",
      " - *fy:* af, btw, fy, half, nv, ren\n",
      " - *starts:* affects, applies, clears, closes, coincides, falls, keeps, lets, means, mins, passes, pays, pts, reaches, regards, reminders, reopens, replies, reps, rumours, safra, shows, sounds, starts, stays, sucks, tells, understands, updates, works\n",
      " - *account:* account, care, diam, fcc, gpmg, input, insight, kena, mark, mask, mth, nsti, occifer, pen, pt, reckon, rt, span, sth, tag, volunteer\n",
      " - *closed:* closed, concerned, confused, coy, disabled, pleased, scan, scared, situp, stuck, worried\n",
      " - *fug:* bmt, boy, dunno, fug, gagt, gng, goodbye, height, hell, hotline, locker, nth, ot, pc, tbh, tio, wan\n",
      " - *fk:* ah, fk, huh, quote, va, xxx\n",
      " - *smart:* absent, appropriate, aware, certain, comfortable, complex, confidential, convenient, easy, effective, eligible, equivalent, fair, friendly, human, indoor, intense, ippt, likely, lucky, muslim, nervous, obvious, popular, positive, private, progressive, prone, proud, random, religious, rude, salty, short, sick, slow, smart, superior, unfair, unlikely, valid, wise, worth\n",
      " - *environment:* admin, age, army, bag, bed, boat, body, book, borderline, cd, commander, company, environment, ffi, group, guy, house, income, joke, loophole, man, marksman, max, nsfs, office, phase, photo, ppls, problem, process, recuit, requirement, salary, scheme, spy, story, thing, thread, thrgh, virus, wallet\n",
      " - *fire:* attendance, batches, board, brain, cert, choice, command, doc, eligibility, email, enlistee, exercise, fire, gg, issue, jiak, liao, list, manpower, norm, policy, reminder, rep, taiji, target, thumb, tone\n",
      " - *qn:* oc, qn, sg\n",
      " - *hearing:* booking, feeling, gathering, hearing, manning, meaning, opening, packing, scheduling, screening, thinking, timing, training, understanding, upping, wellbeing\n",
      " - *hear:* accept, achieve, allow, consider, expect, find, guess, hear, indicate, learn, let, maintain, promote, receive, remember, suggest, tend, try, want, wear\n",
      " - *fulfilled:* agreed, attempted, defaulted, enhanced, failed, fked, fulfilled, guaranteed, mred, phased, submitted, updated, waited, waived, wasted\n",
      " - *perform:* add, affect, apply, appt, attain, bother, bring, cancel, carry, commit, comply, count, create, die, eat, enter, face, fall, fccs, grow, hold, join, leave, lock, pass, perform, pick, play, prepare, provide, push, reach, rec, recover, refer, remove, repeat, respond, rest, reveal, save, serve, shag, shower, smoke, speak, submit, throw, treat, wish\n",
      " - *experts:* boys, companies, doctors, experts, folks, guys, injuries, kinds, men, officers, parents, people, pros, redditors, seniors, sergeants, soldiers, stations, students, superiors, units, verifies\n",
      " - *cock:* arrangement, bot, buddy, cock, covid, duno, enlistment, fate, gradient, grey, handful, keyword, lenient, loh, male, nsportal, obese, ofc, psychiatrist, qns, queue, rank, reservist, sound, tot, trooper, unsure\n",
      " - *deal:* activate, ba, backside, band, batch, benefit, blame, bravo, bunk, check, chill, classify, deal, declare, den, dey, drill, drink, drop, enlist, enrol, goodie, guard, habit, jan, kick, lie, link, log, mate, mindset, offer, opt, ord, outcome, patch, ppl, recourse, reward, ring, slack, stick, talk, train, undergo, vibrate, vote, wash, wat, wave, whack, work\n",
      " - *platoonmate:* allocate, calculate, collect, convert, counter, express, fit, gatecrash, generate, impt, intake, notify, parrot, platoonmate, seperate, sgts, shiong, void\n",
      " - *soc:* ankle, buibui, cadet, chiu, chop, commando, couple, cpf, feb, friend, fulfil, gym, icon, ict, inconvenience, lan, leh, lumber, mat, ndu, offender, oot, soc, tekan, timer, trainee, trainer\n",
      " - *makan:* fiit, makan, probably\n",
      " - *unreasonable:* available, busy, dumb, helpful, infectious, interested, lazy, obligatory, payable, possible, reliable, safe, stupid, tired, unavailable, unreasonable, unsafe, upcoming, useful, usual, yellow\n",
      " - *bleed:* bleed, cdf, dig, fbo, finish, forgot, iq, ive, kang, legit, mp, msg, need, pes, ptp, saikang, sar, suffer, switch, wtf\n",
      " - *steps:* ans, appointments, awards, beards, cents, commences, courses, explosives, fees, games, incentives, lessons, mths, nsmen, ops, points, statistics, steps, thou, timings, tips, tons, weekdays, windows\n",
      " - *water:* aiya, appointment, banner, birthday, branch, breakfast, callup, camera, car, cash, cat, category, certificate, citizen, cmi, color, computer, consultation, credit, duration, duty, family, fuel, home, image, line, lunch, machine, majority, ministry, mri, news, oct, offence, paper, pity, program, road, schedule, school, shirt, siam, state, sun, university, video, war, water, website, weight\n",
      " - *whichever:* whichever\n",
      " - *foot:* attire, bdae, burden, button, camp, chain, colour, foot, google, govt, guardroom, hand, head, jin, jun, level, llst, lor, maju, mind, motorbike, ocs, pack, paradise, paynow, pouch, prob, protocol, reply, rifle, rsaf, satisfy, scgp, scope, search, share, simi, sop, speed, store, supply, title, toilet, touch, type, yrs\n",
      " - *cpl:* alr, cpl, defaulter, ie, malays, medic, plenty, swee, tmr, zzz\n",
      " - *planning:* assuming, clearing, closing, counting, facing, meeting, passing, planning, reaching, reopening, risking, sleeping, taking, undergoing, warning, wondering\n",
      " - *purposes:* activities, balls, cadets, cancellations, changes, comments, congrats, consequences, cycles, events, excuses, experiences, guards, hands, lines, locations, macdonals, mates, mcdonalds, measures, obligations, orns, packs, pants, peers, pictures, places, prayers, purposes, recruits, results, rts, scholarships, sessions, skills, sums, tests, thurs, ups\n",
      " - *pretend:* agree, appreciate, attend, bmi, choose, confirm, continue, decide, detect, disrupt, extend, fail, follow, handle, hate, ignore, imagine, intend, ish, mean, mention, miss, pretend, prevent, profile, prove, realise, recommend, reflect, reject, relate, seek, start, stay, suck, understand, vary, wake, watch, welcome, workyear\n",
      " - *tuition:* accident, aerobics, anybody, area, article, brother, bus, culture, distance, doctor, employer, equipment, everybody, football, game, kranji, liability, lifestyle, lot, mail, mc, memo, message, nation, number, officer, person, phone, platform, security, service, term, tuition\n",
      " - *research:* approach, auto, award, balance, basis, combat, confirmation, cycle, deadline, defence, diff, driver, education, entrance, exception, feedback, fitness, flag, infantry, injury, instructor, interview, lack, location, loss, maintenance, march, nonsense, pace, page, pain, picture, platoon, print, promotion, quota, reschedule, research, response, resumption, risk, safety, score, selection, shape, spot, stadium, step, stuff, track, vacancy, window, workout, wsdip\n",
      " - *sch:* abt, ar, bu, ck, damn, ic, la, lk, okay, osa, sch, ur, wa\n",
      " - *jc:* bp, jc, lo\n",
      " - *parliament:* cisco, clerk, cmpb, expert, fark, government, hair, heng, instruction, khatib, min, morale, nsf, parliament, regimentation, relief, shelter, specialist, student\n",
      " - *qualifies:* allowances, answers, attempts, bags, bookouts, breaks, bunks, burpees, charges, colours, commandos, conducts, counts, dates, defines, doubts, exercises, forms, gloves, icts, knots, lectures, matters, medals, muslims, outfields, peeps, plans, pullups, qualifies, records, regulars, reports, reviews, rewards, rounds, sets, sibei, slots, sms, stars, states, workouts\n",
      " - *begins:* allows, begins, depends, ends, fails, feels, happens, isnt, knows, needs, opens\n",
      " - *fare:* aug, db, define, discharge, endorse, enquire, escalate, escape, fare, february, fuck, fulfill, highlight, hrs, kenna, le, lead, mo, natalie, newbie, nightmare, nvm, oic, peer, pew, pm, post, shoot, skip, slip, tick, trick, xiong\n"
     ]
    }
   ],
   "source": [
    "word_array = np.array(words)\n",
    "for cluster_id in np.unique(affprop.labels_):\n",
    "    exemplar = word_array[affprop.cluster_centers_indices_[cluster_id]]\n",
    "    cluster = np.unique(word_array[np.nonzero(affprop.labels_==cluster_id)])\n",
    "    cluster_str = \", \".join(cluster)\n",
    "    print(\" - *%s:* %s\" % (exemplar, cluster_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "- Silhouette score\n",
    "- Calinski Harabasz index\n",
    "- Davies Bouldin index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score: 0.025242906\n",
      "Calinski Harabasz: 28.82584207818853\n",
      "Davies Bouldin: 2.91456112363272\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Silhouette score:\", metrics.silhouette_score(X, affprop.labels_, metric='euclidean'))\n",
    "print(\"Calinski Harabasz:\", metrics.calinski_harabasz_score(X, affprop.labels_))\n",
    "print(\"Davies Bouldin:\", metrics.davies_bouldin_score(X, affprop.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 GloVe's Pretrained Vectors + Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress bar\n",
    "def progress(count, total, status=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "    percents = round(100.0 * count / float(total), 1)\n",
    "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n",
    "    sys.stdout.flush() \n",
    "\n",
    "# load dictionary of word vectors based on pretrained Glove model\n",
    "def loadGloveDict(File):\n",
    "    print(\"Loading glove model\")\n",
    "    f = open(File, 'r', encoding = 'utf-8')\n",
    "    gloveDict = {}\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        progress(i, 400000, status = 'retreiving vectors')\n",
    "        wordEmbedding = pd.DataFrame([float(value) for value in splitLine[1:]]).T\n",
    "        gloveDict[word] = wordEmbedding\n",
    "        i += 1\n",
    "    print(len(gloveDict), \"words loaded\")\n",
    "    return gloveDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading glove model\n",
      "[=================-------------------------------------------] 29.0% ...retreiving vectors\r"
     ]
    }
   ],
   "source": [
    "model = loadGloveDict(\"../glove.6B/glove.6B.300d.txt\") #pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeatures = len(words_df['index'])\n",
    "vectorlist = []\n",
    "notInCorpus = []\n",
    "\n",
    "for i in range(0, numFeatures):\n",
    "    progress(i, numFeatures - 1, status = \"concatenating extracted vectors\")\n",
    "    wordAsDF = words_df['index'][[i]]\n",
    "    try:\n",
    "        vector = pd.concat([wordAsDF, model[wordAsDF[i]].set_index(wordAsDF.index)], axis = 1)\n",
    "        vectorlist.append(vector)\n",
    "    except KeyError:\n",
    "        notInCorpus.append(wordAsDF[i])\n",
    "\n",
    "embeddings = pd.concat(vectorlist).reset_index(drop = True).rename(columns = {\"index\":\"feature\"})\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notInCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_X = StandardScaler()\n",
    "X_glove = sc_X.fit_transform(embeddings.iloc[:,1:])\n",
    "X_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affprop_glove = AffinityPropagation()\n",
    "affprop_glove.fit(X_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_array = np.array(words)\n",
    "for cluster_id in np.unique(affprop_glove.labels_):\n",
    "    exemplar = word_array[affprop_glove.cluster_centers_indices_[cluster_id]]\n",
    "    cluster = np.unique(word_array[np.nonzero(affprop_glove.labels_ == cluster_id)])\n",
    "    cluster_str = \", \".join(cluster)\n",
    "    print(\" - *%s:* %s\" % (exemplar, cluster_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
