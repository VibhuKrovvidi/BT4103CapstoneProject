{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 10.6MB/s]                    \n",
      "2021-03-03 13:52:18 INFO: Downloading default packages for language: en (English)...\n",
      "2021-03-03 13:52:20 INFO: File exists: /Users/vibhukrovvidi/stanza_resources/en/default.zip.\n",
      "2021-03-03 13:52:24 INFO: Finished downloading models and saved to /Users/vibhukrovvidi/stanza_resources.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vibhukrovvidi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vibhukrovvidi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/vibhukrovvidi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "import stanza\n",
    "stanza.download('en') # download English model\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(txt, nlp):\n",
    "    try:\n",
    "        txt = txt.lower()\n",
    "    except:\n",
    "        pass;\n",
    "\n",
    "    sentList = nltk.sent_tokenize(txt)\n",
    "\n",
    "    retlist = [];\n",
    "    \n",
    "    for line in sentList:\n",
    "        txt_list = nltk.word_tokenize(line)\n",
    "        taggedList = nltk.pos_tag(txt_list)\n",
    "        \n",
    "        newwordList = []\n",
    "        flag = 0\n",
    "        for i in range(0,len(taggedList)-1):\n",
    "            if(taggedList[i][1]==\"NN\" and taggedList[i+1][1]==\"NN\"):\n",
    "                newwordList.append(taggedList[i][0]+taggedList[i+1][0])\n",
    "                flag=1\n",
    "            else:\n",
    "                if(flag==1):\n",
    "                    flag=0\n",
    "                    continue\n",
    "                newwordList.append(taggedList[i][0])\n",
    "                if(i==len(taggedList)-2):\n",
    "                    newwordList.append(taggedList[i+1][0])\n",
    "        finaltxt = ' '.join(word for word in newwordList)\n",
    "    \n",
    "    \n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        new_txt_list = nltk.word_tokenize(finaltxt)\n",
    "        wordsList = [w for w in new_txt_list if not w in stop_words]\n",
    "        taggedList = nltk.pos_tag(wordsList)\n",
    "        \n",
    "        doc = nlp(finaltxt)\n",
    "        dep_node = []\n",
    "        try:\n",
    "            for dep_edge in doc.sentences[0].dependencies:\n",
    "                dep_node.append([dep_edge[2].text, dep_edge[0].id, dep_edge[1]])\n",
    "            for i in range(0, len(dep_node)):\n",
    "                if (int(dep_node[i][1]) != 0):\n",
    "                    dep_node[i][1] = newwordList[(int(dep_node[i][1]) - 1)]\n",
    "        except:\n",
    "            pass;\n",
    "        \n",
    "        #print(dep_node)\n",
    "        \n",
    "        featureList = []\n",
    "        categories = []\n",
    "        for i in taggedList:\n",
    "            if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "                featureList.append(list(i))\n",
    "                categories.append(i[0])\n",
    "        #print(featureList)\n",
    "        #print(categories)\n",
    "        \n",
    "        \n",
    "        fcluster = []\n",
    "        for i in featureList:\n",
    "            filist = []\n",
    "            for j in dep_node:\n",
    "                if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "                    if(j[0]==i[0]):\n",
    "                        filist.append(j[1])\n",
    "                    else:\n",
    "                        filist.append(j[0])\n",
    "            fcluster.append([i[0], filist])\n",
    "        print(fcluster) \n",
    "        \n",
    "        # Remove all features with no sentiment word:\n",
    "        \n",
    "        retlist.append(fcluster)\n",
    "    return retlist;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_extraction(df, nlp, content_str = \"Content\"):\n",
    "    idx = 0;\n",
    "    review_list = df[content_str].to_list()\n",
    "    feat_count = dict()\n",
    "    feat_sent = dict()\n",
    "    #nlp = stanza.Pipeline('en')\n",
    "    \n",
    "    # Replace \"\" with nan's for removal\n",
    "    df[content_str].replace('', np.nan, inplace=True)\n",
    "    df.dropna(subset=[content_str], inplace=True)\n",
    "    print(\" Processing : \" , df.shape[0], \"rows of data\")\n",
    "    for review in review_list:\n",
    "        print(\"Review Number : \", idx);\n",
    "        idx += 1;\n",
    "        if idx >= df.shape[0]:\n",
    "            break;\n",
    "        try:\n",
    "            output = feature_extraction(review, nlp);\n",
    "        except:\n",
    "            pass;\n",
    "        for sent in output:\n",
    "            for pair in sent:\n",
    "                print(pair)\n",
    "                if pair[0] in feat_sent:\n",
    "                    if pair[1] is not None:\n",
    "                        flist = feat_sent[pair[0]]\n",
    "                        if isinstance(pair[1], list):\n",
    "                            for i in pair[1]:\n",
    "                                flist.append(i)\n",
    "                        else:\n",
    "                            flist.append(pair[1])\n",
    "                        feat_sent[pair[0]] = flist;\n",
    "                else:\n",
    "                    if pair[1] is not None:\n",
    "                        flist = pair[1]\n",
    "                    else:\n",
    "                        flist = list()\n",
    "                    feat_sent[pair[0]] = flist;\n",
    "                \n",
    "                if pair[0] in feat_count:\n",
    "                    feat_count[pair[0]] = feat_count[pair[0]] + 1;\n",
    "                else:\n",
    "                    feat_count[pair[0]] = 1\n",
    "\n",
    "    #print(feat_count);\n",
    "    return feat_count, feat_sent;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(a, b, nlp):\n",
    "\n",
    "    sentiment_score = dict()\n",
    "\n",
    "    # Delete features with no descriptors\n",
    "    cob = b.copy()\n",
    "    for feat in cob.keys():\n",
    "        #print(cob[feat])\n",
    "        if cob[feat] == []:\n",
    "            del b[feat]\n",
    "\n",
    "    # Run pre-built sentiment score and take avg of all descriptors\n",
    "    for f in b.keys():\n",
    "        #print(f);\n",
    "        ssum = 0;\n",
    "        for g in b[f]:\n",
    "            try:\n",
    "                doc = nlp(g);\n",
    "\n",
    "                for i in doc.sentences:\n",
    "\n",
    "                        #print(i.sentiment)\n",
    "                        ssum += i.sentiment;\n",
    "            except:\n",
    "                pass;\n",
    "\n",
    "        sentiment_score[f] = ssum / len(b[f])\n",
    "\n",
    "        adf = pd.DataFrame.from_dict(a, orient='index', columns=['Freq'])\n",
    "    adf.sort_values(by=\"Freq\", ascending=False, inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "    avg_sent = pd.DataFrame.from_dict(sentiment_score, orient='index', columns=[\"Avg_sent\"])\n",
    "\n",
    "    final_sent = avg_sent.merge(adf, left_index=True, right_index=True)\n",
    "    final_sent.sort_values(by=\"Freq\", ascending=False, inplace=True)\n",
    "    return final_sent;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-03 14:13:43 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-03-03 14:13:43 INFO: Use device: cpu\n",
      "2021-03-03 14:13:43 INFO: Loading: tokenize\n",
      "2021-03-03 14:13:43 INFO: Loading: pos\n",
      "2021-03-03 14:13:43 INFO: Loading: lemma\n",
      "2021-03-03 14:13:43 INFO: Loading: depparse\n",
      "2021-03-03 14:13:44 INFO: Loading: sentiment\n",
      "2021-03-03 14:13:45 INFO: Loading: ner\n",
      "2021-03-03 14:13:47 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing :  52 rows of data\n",
      "Review Number :  0\n",
      "[['professional', ['very', 'people']], ['people', ['professional', 'there']]]\n",
      "[['recommended⭐⭐⭐⭐⭐', ['kind']], ['patient', ['very', 'kind']], ['kind', ['recommended⭐⭐⭐⭐⭐', 'patient', 'respectful']], ['respectful', ['kind']], ['..', []], ['smooth', ['very']], ['medical', [14]], ['check-up', []]]\n",
      "['professional', ['very', 'people']]\n",
      "['people', ['professional', 'there']]\n",
      "['recommended⭐⭐⭐⭐⭐', ['kind']]\n",
      "['patient', ['very', 'kind']]\n",
      "['kind', ['recommended⭐⭐⭐⭐⭐', 'patient', 'respectful']]\n",
      "['respectful', ['kind']]\n",
      "['..', []]\n",
      "['smooth', ['very']]\n",
      "['medical', [14]]\n",
      "['check-up', []]\n",
      "Review Number :  1\n",
      "[['reviews', ['other', 'suggest']], ['people', ['friendly', 'here']], ['adequately', ['friendly']], ['friendly', ['people', 'adequately']], ['anywhere', ['else']], ['else', ['anywhere']], ['sg', []]]\n",
      "[['place', ['clean']], ['really', ['clean']], ['clean', ['place', 'really']], ['efficient', []]]\n",
      "['reviews', ['other', 'suggest']]\n",
      "['people', ['friendly', 'here']]\n",
      "['adequately', ['friendly']]\n",
      "['friendly', ['people', 'adequately']]\n",
      "['anywhere', ['else']]\n",
      "['else', ['anywhere']]\n",
      "['sg', []]\n",
      "['place', ['clean']]\n",
      "['really', ['clean']]\n",
      "['clean', ['place', 'really']]\n",
      "['efficient', []]\n",
      "Review Number :  2\n",
      "[['place', ['fine']], ['overall', ['fine']], ['fine', ['place', 'overall']]]\n",
      "[['good', ['experience']], ['experience', ['good']]]\n",
      "[['tip', ['just']], ['u', []], ['medical', ['checkup']], ['checkup', ['medical']]]\n",
      "[['dont', []], ['late', ['go']], ['else', []], ['u', ['have']], ['back', ['come']], ['day', ['other']], ['complete', ['rest']], ['rest', ['complete']]]\n",
      "['place', ['fine']]\n",
      "['overall', ['fine']]\n",
      "['fine', ['place', 'overall']]\n",
      "['good', ['experience']]\n",
      "['experience', ['good']]\n",
      "['tip', ['just']]\n",
      "['u', []]\n",
      "['medical', ['checkup']]\n",
      "['checkup', ['medical']]\n",
      "['dont', []]\n",
      "['late', ['go']]\n",
      "['else', []]\n",
      "['u', ['have']]\n",
      "['back', ['come']]\n",
      "['day', ['other']]\n",
      "['complete', ['rest']]\n",
      "['rest', ['complete']]\n",
      "Review Number :  3\n",
      "[['nscheckup', []], ['checkuptoday', []], ['august', ['...']], ['gate', [':']], ['entrance', []], ['securitycheck', []], ['counter', []], ['stickerpass', ['a']], ['walk', []], ['gate', [':']], [\"n't\", ['...']], ['thinking', []], ['scan', []], ['…', []]]\n",
      "['nscheckup', []]\n",
      "['checkuptoday', []]\n",
      "['august', ['...']]\n",
      "['gate', [':']]\n",
      "['entrance', []]\n",
      "['securitycheck', []]\n",
      "['counter', []]\n",
      "['stickerpass', ['a']]\n",
      "['walk', []]\n",
      "['gate', [':']]\n",
      "[\"n't\", ['...']]\n",
      "['thinking', []]\n",
      "['scan', []]\n",
      "['…', []]\n",
      "Review Number :  4\n",
      "[[\"n't\", ['bother']], ['bother', [\"n't\", 'showing']], ['timing', []], ['hours', []], ['last', ['person']], ['person', ['you', 'last']], ['line', []]]\n",
      "[['doctors', ['care']], [\"n't\", ['care', 'have']], ['really', ['care', 'have']], ['fair', ['which']], [\"n't\", ['care', 'have']], ['really', ['care', 'have']], ['choice', ['have']]]\n",
      "[['place', ['waste']], ['complete', ['waste']], ['waste', ['place', 'complete']], ['space', []], ['time', []]]\n",
      "[\"n't\", ['bother']]\n",
      "['bother', [\"n't\", 'showing']]\n",
      "['timing', []]\n",
      "['hours', []]\n",
      "['last', ['person']]\n",
      "['person', ['you', 'last']]\n",
      "['line', []]\n",
      "['doctors', ['care']]\n",
      "[\"n't\", ['care', 'have']]\n",
      "['really', ['care', 'have']]\n",
      "['fair', ['which']]\n",
      "[\"n't\", ['care', 'have']]\n",
      "['really', ['care', 'have']]\n",
      "['choice', ['have']]\n",
      "['place', ['waste']]\n",
      "['complete', ['waste']]\n",
      "['waste', ['place', 'complete']]\n",
      "['space', []]\n",
      "['time', []]\n",
      "Review Number :  5\n",
      "[['nsf', ['reviews']], ['reviews', ['nsf']], ['lol', []]]\n",
      "['nsf', ['reviews']]\n",
      "['reviews', ['nsf']]\n",
      "['lol', []]\n",
      "Review Number :  6\n",
      "[['extremely', ['long']], ['long', ['extremely', 'time']], ['time', ['long', 'takes']], ['due', []], ['waiting', []]]\n",
      "[['overall', ['waste']], ['complete', ['waste']], ['waste', ['overall', 'complete']], ['time', []]]\n",
      "['extremely', ['long']]\n",
      "['long', ['extremely', 'time']]\n",
      "['time', ['long', 'takes']]\n",
      "['due', []]\n",
      "['waiting', []]\n",
      "['overall', ['waste']]\n",
      "['complete', ['waste']]\n",
      "['waste', ['overall', 'complete']]\n",
      "['time', []]\n",
      "Review Number :  7\n",
      "[['staff', ['professional']], ['professional', ['staff']], ['knows', []]]\n",
      "[['idk', []], ['bad', ['reviews']], ['personal', ['experience']], ['experience', ['personal']], ['everyone', ['is', 'there']], ['helpful', ['very', 'is']], ['initiative', ['take']], ['help', ['me']]]\n",
      "['staff', ['professional']]\n",
      "['professional', ['staff']]\n",
      "['knows', []]\n",
      "['idk', []]\n",
      "['bad', ['reviews']]\n",
      "['personal', ['experience']]\n",
      "['experience', ['personal']]\n",
      "['everyone', ['is', 'there']]\n",
      "['helpful', ['very', 'is']]\n",
      "['initiative', ['take']]\n",
      "['help', ['me']]\n",
      "Review Number :  8\n",
      "[['unfriendly', ['staff']], ['staff', ['unfriendly']]]\n",
      "[['guards', ['doing']], ['job', ['doing']], ['staff', ['keep']], ['stuff', ['more', 'do', 'suppose']], ['suppose', ['they', 'not', 'stuff']]]\n",
      "[['absolutely', ['atrocious']], ['atrocious', ['absolutely']]]\n",
      "[['woman', ['is']], ['keeps', ['who', 'vac', 'changing']], ['tone', ['changing']], ['talks', ['she']], ['people', ['threatens']], ['people', ['threatens']]]\n",
      "[['always', ['removes']], ['mask', ['removes']], ['talk', []], ['people', []], ['expressions', ['show']]]\n",
      "['unfriendly', ['staff']]\n",
      "['staff', ['unfriendly']]\n",
      "['guards', ['doing']]\n",
      "['job', ['doing']]\n",
      "['staff', ['keep']]\n",
      "['stuff', ['more', 'do', 'suppose']]\n",
      "['suppose', ['they', 'not', 'stuff']]\n",
      "['absolutely', ['atrocious']]\n",
      "['atrocious', ['absolutely']]\n",
      "['woman', ['is']]\n",
      "['keeps', ['who', 'vac', 'changing']]\n",
      "['tone', ['changing']]\n",
      "['talks', ['she']]\n",
      "['people', ['threatens']]\n",
      "['people', ['threatens']]\n",
      "['always', ['removes']]\n",
      "['mask', ['removes']]\n",
      "['talk', []]\n",
      "['people', []]\n",
      "['expressions', ['show']]\n",
      "Review Number :  9\n",
      "[['staff', ['rude']], ['medical', ['screening']], ['screening', ['medical', 'station']], ['station', ['screening']], ['height', []], ['weight', []], ['extremely', ['rude']], ['rude', ['staff', 'extremely']], ['unfriendly', []]]\n",
      "['staff', ['rude']]\n",
      "['medical', ['screening']]\n",
      "['screening', ['medical', 'station']]\n",
      "['station', ['screening']]\n",
      "['height', []]\n",
      "['weight', []]\n",
      "['extremely', ['rude']]\n",
      "['rude', ['staff', 'extremely']]\n",
      "['unfriendly', []]\n",
      "Review Number :  10\n",
      "[['staff', ['serious']], ['serious', ['staff', 'not']], ['impatient', []]]\n",
      "[['undesirably', ['long']], ['long', ['undesirably', 'times']], ['times', ['long', 'waiting']]]\n",
      "[['cmpb', ['recommend']], ['friend', []]]\n",
      "['staff', ['serious']]\n",
      "['serious', ['staff', 'not']]\n",
      "['impatient', []]\n",
      "['undesirably', ['long']]\n",
      "['long', ['undesirably', 'times']]\n",
      "['times', ['long', 'waiting']]\n",
      "['cmpb', ['recommend']]\n",
      "['friend', []]\n",
      "Review Number :  11\n",
      "[['sent', ['just', 'son']], ['son', ['sent']], ['preenlistment', ['enlistmentcheckup']], ['enlistmentcheckup', ['preenlistment']], ['morning', []]]\n",
      "[['guard', ['give']], ['entrance', []], [\"n't\", ['give']], ['clear', ['instructions']], ['instructions', ['clear', 'give']]]\n",
      "[['son', ['got']], ['alight', ['got']], [\"n't\", ['drive']]]\n",
      "[['helloplease', []], ['train', ['army']], ['army…', []]]\n",
      "['sent', ['just', 'son']]\n",
      "['son', ['sent']]\n",
      "['preenlistment', ['enlistmentcheckup']]\n",
      "['enlistmentcheckup', ['preenlistment']]\n",
      "['morning', []]\n",
      "['guard', ['give']]\n",
      "['entrance', []]\n",
      "[\"n't\", ['give']]\n",
      "['clear', ['instructions']]\n",
      "['instructions', ['clear', 'give']]\n",
      "['son', ['got']]\n",
      "['alight', ['got']]\n",
      "[\"n't\", ['drive']]\n",
      "['helloplease', []]\n",
      "['train', ['army']]\n",
      "['army…', []]\n",
      "Review Number :  12\n",
      "[['inconvenient', ['most', 'locations']], ['locations', ['inconvenient', 'seen']], ['ever', ['seen']]]\n",
      "[['terrible', ['directions']], ['directions', ['terrible']]]\n",
      "[['rude', ['staff']], ['staff', ['rude']]]\n",
      "[['expect', ['process']], ['whole', ['process']], ['process', ['whole', 'expect']], ['take', ['hours']], ['hours', ['take']]]\n",
      "[['%', ['recommended']]]\n",
      "['inconvenient', ['most', 'locations']]\n",
      "['locations', ['inconvenient', 'seen']]\n",
      "['ever', ['seen']]\n",
      "['terrible', ['directions']]\n",
      "['directions', ['terrible']]\n",
      "['rude', ['staff']]\n",
      "['staff', ['rude']]\n",
      "['expect', ['process']]\n",
      "['whole', ['process']]\n",
      "['process', ['whole', 'expect']]\n",
      "['take', ['hours']]\n",
      "['hours', ['take']]\n",
      "['%', ['recommended']]\n",
      "Review Number :  13\n",
      "[['ok', []], ['lah', []], ['review', []], ['visit', []], ['january', []], ['maybe', ['sikit']], ['sikit', ['so', 'maybe']], ['date', []]]\n",
      "[['nsf', ['staff']], ['staff', ['nsf', 'ok']], ['ok', ['staff']], ['typical', ['bochap']], ['bochap', ['typical']], ['happy', []], ['bird', []], ['tio', ['switch', 'vocation']], ['switch', ['tio']], ['vocation', ['tio']]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mo', ['seemed']], ['hand', ['other']], ['si', []], ['pehbuay', ['buaysong']], ['buaysong', ['pehbuay', 'seemed']]]\n",
      "[['probably', ['this']], ['…', []]]\n",
      "['ok', []]\n",
      "['lah', []]\n",
      "['review', []]\n",
      "['visit', []]\n",
      "['january', []]\n",
      "['maybe', ['sikit']]\n",
      "['sikit', ['so', 'maybe']]\n",
      "['date', []]\n",
      "['nsf', ['staff']]\n",
      "['staff', ['nsf', 'ok']]\n",
      "['ok', ['staff']]\n",
      "['typical', ['bochap']]\n",
      "['bochap', ['typical']]\n",
      "['happy', []]\n",
      "['bird', []]\n",
      "['tio', ['switch', 'vocation']]\n",
      "['switch', ['tio']]\n",
      "['vocation', ['tio']]\n",
      "['mo', ['seemed']]\n",
      "['hand', ['other']]\n",
      "['si', []]\n",
      "['pehbuay', ['buaysong']]\n",
      "['buaysong', ['pehbuay', 'seemed']]\n",
      "['probably', ['this']]\n",
      "['…', []]\n",
      "Review Number :  14\n",
      "[['tbh', []], ['bad', ['not', 'that']], ['place', []], ['visit', []], ['contrary', []], ['others', ['saying']]]\n",
      "['tbh', []]\n",
      "['bad', ['not', 'that']]\n",
      "['place', []]\n",
      "['visit', []]\n",
      "['contrary', []]\n",
      "['others', ['saying']]\n",
      "Review Number :  15\n",
      "[['enlisting', ['here']], ['sure', ['make']], ['medical', ['conditions']], ['conditions', ['medical', 'declare']]]\n",
      "[['severe', []], ['minor', []], ['medical', ['officer']], ['officer', ['medical']], ['checkup', []]]\n",
      "[['believe', ['you', 'not']], ['well', ['fare']], ['combat/pes-fit', []], ['bmt', []], ['medical', ['…', 'specialistletter']], ['specialistletter', ['medical']], ['…', ['medical']]]\n",
      "['enlisting', ['here']]\n",
      "['sure', ['make']]\n",
      "['medical', ['conditions']]\n",
      "['conditions', ['medical', 'declare']]\n",
      "['severe', []]\n",
      "['minor', []]\n",
      "['medical', ['officer']]\n",
      "['officer', ['medical']]\n",
      "['checkup', []]\n",
      "['believe', ['you', 'not']]\n",
      "['well', ['fare']]\n",
      "['combat/pes-fit', []]\n",
      "['bmt', []]\n",
      "['medical', ['…', 'specialistletter']]\n",
      "['specialistletter', ['medical']]\n",
      "['…', ['medical']]\n",
      "Review Number :  16\n",
      "[['guards', ['rude']], ['rude', ['guards', 'very']]]\n",
      "[['ask', ['question']], ['question', ['ask', 'ignore']], ['ignore', ['they', 'question', 'you']]]\n",
      "[['rest', ['nice']], ['staff', []], ['nice', ['rest', 'though']], ['friendly', []]]\n",
      "['guards', ['rude']]\n",
      "['rude', ['guards', 'very']]\n",
      "['ask', ['question']]\n",
      "['question', ['ask', 'ignore']]\n",
      "['ignore', ['they', 'question', 'you']]\n",
      "['rest', ['nice']]\n",
      "['staff', []]\n",
      "['nice', ['rest', 'though']]\n",
      "['friendly', []]\n",
      "Review Number :  17\n",
      "[['overall', ['experience']], ['great', ['experience']], ['experience', ['overall', 'great']], ['medic', ['professional']], ['professional', ['medic']], ['blooddraw', []]]\n",
      "['overall', ['experience']]\n",
      "['great', ['experience']]\n",
      "['experience', ['overall', 'great']]\n",
      "['medic', ['professional']]\n",
      "['professional', ['medic']]\n",
      "['blooddraw', []]\n",
      "Review Number :  18\n",
      "[['kind', ['very', 'people']], ['people', ['kind']], ['cmpd', []], ['medical', ['check']], ['check', ['medical', 'up']]]\n",
      "[['constantly', ['greeted']], ['smile', []], ['patience', []]]\n",
      "['kind', ['very', 'people']]\n",
      "['people', ['kind']]\n",
      "['cmpd', []]\n",
      "['medical', ['check']]\n",
      "['check', ['medical', 'up']]\n",
      "['constantly', ['greeted']]\n",
      "['smile', []]\n",
      "['patience', []]\n",
      "Review Number :  19\n",
      "[[\"n't\", ['bully']], ['bully', [\"n't\", 'me']]]\n",
      "[\"n't\", ['bully']]\n",
      "['bully', [\"n't\", 'me']]\n",
      "Review Number :  20\n",
      "[['meh', []]]\n",
      "[['staff', ['nice']], ['pretty', ['nice']], ['nice', ['staff', 'pretty']]]\n",
      "['meh', []]\n",
      "['staff', ['nice']]\n",
      "['pretty', ['nice']]\n",
      "['nice', ['staff', 'pretty']]\n",
      "Review Number :  21\n",
      "[['lousy', ['dk']], ['service', ['dk']], ['dk', ['lousy', 'service', 'help']], ['help', ['dk', 'people']], ['people', ['help']]]\n",
      "[['ask', ['question', 'taiji']], ['question', ['ask']], ['also', ['say']], ['dont', []], ['ask', ['question', 'taiji']], ['taiji', ['not', 'ask']]]\n",
      "['lousy', ['dk']]\n",
      "['service', ['dk']]\n",
      "['dk', ['lousy', 'service', 'help']]\n",
      "['help', ['dk', 'people']]\n",
      "['people', ['help']]\n",
      "['ask', ['question', 'taiji']]\n",
      "['question', ['ask']]\n",
      "['also', ['say']]\n",
      "['dont', []]\n",
      "['ask', ['question', 'taiji']]\n",
      "['taiji', ['not', 'ask']]\n",
      "Review Number :  22\n",
      "[['[', []], [']', []], ['tuesday', []], ['negative', ['reviews']], ['reviews', ['negative']], ['share', ['i', 'opinion']], ['quick', ['opinion']], ['personal', ['opinion']], ['opinion', ['quick', 'personal', 'share']]]\n",
      "[['perhaps', ['varies']], ['varies', ['perhaps', 'it']], ['person', []], ['person', []], ['trip', ['great']], ['cmpb', []], ['ultimately', ['great']], ['great', ['trip', 'ultimately']], ['definitely', ['experience']], ['memorable', ['experience']], ['experience', ['definitely', 'memorable']]]\n",
      "[['medical', []], ['…', []]]\n",
      "['[', []]\n",
      "[']', []]\n",
      "['tuesday', []]\n",
      "['negative', ['reviews']]\n",
      "['reviews', ['negative']]\n",
      "['share', ['i', 'opinion']]\n",
      "['quick', ['opinion']]\n",
      "['personal', ['opinion']]\n",
      "['opinion', ['quick', 'personal', 'share']]\n",
      "['perhaps', ['varies']]\n",
      "['varies', ['perhaps', 'it']]\n",
      "['person', []]\n",
      "['person', []]\n",
      "['trip', ['great']]\n",
      "['cmpb', []]\n",
      "['ultimately', ['great']]\n",
      "['great', ['trip', 'ultimately']]\n",
      "['definitely', ['experience']]\n",
      "['memorable', ['experience']]\n",
      "['experience', ['definitely', 'memorable']]\n",
      "['medical', []]\n",
      "['…', []]\n",
      "Review Number :  23\n",
      "[['medical', ['check']], ['check', ['medical', 'place']], ['place', ['check']], ['saf', []]]\n",
      "['medical', ['check']]\n",
      "['check', ['medical', 'place']]\n",
      "['place', ['check']]\n",
      "['saf', []]\n",
      "Review Number :  24\n",
      "[['others', []], ['dirt', []]]\n",
      "['others', []]\n",
      "['dirt', []]\n",
      "Review Number :  25\n",
      "[['pre-enlistment', ['screening']], ['screening', ['pre-enlistment']], ['sessions', ['few', 'counselling']], ['answer', ['charge']], ['charge', ['answer']]]\n",
      "[['inconvenient', ['location']], ['location', ['inconvenient']]]\n",
      "['pre-enlistment', ['screening']]\n",
      "['screening', ['pre-enlistment']]\n",
      "['sessions', ['few', 'counselling']]\n",
      "['answer', ['charge']]\n",
      "['charge', ['answer']]\n",
      "['inconvenient', ['location']]\n",
      "['location', ['inconvenient']]\n",
      "Review Number :  26\n",
      "[['cookhouse', []]]\n",
      "[['nsf', ['need']], ['meagre', ['pay']], ['pay', ['meagre']]]\n",
      "['cookhouse', []]\n",
      "['nsf', ['need']]\n",
      "['meagre', ['pay']]\n",
      "['pay', ['meagre']]\n",
      "Review Number :  27\n",
      "[['officerattitude', ['good']], ['good', ['officerattitude', 'not']], ['patience', []], ['service', []]]\n",
      "[[\"n't\", ['understand']], ['use', ['he', 'tone']], ['unfriendly', ['tone']], ['tone', ['unfriendly', 'use']], ['repeat', []], ['language', []]]\n",
      "['officerattitude', ['good']]\n",
      "['good', ['officerattitude', 'not']]\n",
      "['patience', []]\n",
      "['service', []]\n",
      "[\"n't\", ['understand']]\n",
      "['use', ['he', 'tone']]\n",
      "['unfriendly', ['tone']]\n",
      "['tone', ['unfriendly', 'use']]\n",
      "['repeat', []]\n",
      "['language', []]\n",
      "Review Number :  28\n",
      "[['idk', []], ['many', ['so', 'people']], ['people', ['many', 'give']], ['negative', ['reviews']], ['reviews', ['negative', 'give']], ['medical', ['check']], ['check', ['medical', 'up']], ['staff', ['friendly']], ['friendly', ['staff']], ['nsf', ['cool']], ['cool', ['nsf']], ['overall', ['had']], ['good', ['experience']], ['experience', ['good', 'had', 'there']]]\n",
      "['idk', []]\n",
      "['many', ['so', 'people']]\n",
      "['people', ['many', 'give']]\n",
      "['negative', ['reviews']]\n",
      "['reviews', ['negative', 'give']]\n",
      "['medical', ['check']]\n",
      "['check', ['medical', 'up']]\n",
      "['staff', ['friendly']]\n",
      "['friendly', ['staff']]\n",
      "['nsf', ['cool']]\n",
      "['cool', ['nsf']]\n",
      "['overall', ['had']]\n",
      "['good', ['experience']]\n",
      "['experience', ['good', 'had', 'there']]\n",
      "Review Number :  29\n",
      "[['gold', ['star']], ['star', ['gold']], ['public', ['service']], ['service', ['public']]]\n",
      "['gold', ['star']]\n",
      "['star', ['gold']]\n",
      "['public', ['service']]\n",
      "['service', ['public']]\n",
      "Review Number :  30\n",
      "[['form', []], ['meeting', []]]\n",
      "[['even', ['bored']], ['really', ['bored']]]\n",
      "[['cold', ['conditioning']], ['air', ['conditioning']], ['wifi', []]]\n",
      "['form', []]\n",
      "['meeting', []]\n",
      "['even', ['bored']]\n",
      "['really', ['bored']]\n",
      "['cold', ['conditioning']]\n",
      "['air', ['conditioning']]\n",
      "['wifi', []]\n",
      "Review Number :  31\n",
      "[['accessible', ['not']]]\n",
      "['accessible', ['not']]\n",
      "Review Number :  32\n",
      "[['extremely', ['poor']], ['poor', ['extremely', 'customerservice']], ['rude', []], ['customerservice', ['poor']]]\n",
      "['extremely', ['poor']]\n",
      "['poor', ['extremely', 'customerservice']]\n",
      "['rude', []]\n",
      "['customerservice', ['poor']]\n",
      "Review Number :  33\n",
      "[['worst', ['day']], ['day', ['worst']], ['life', []]]\n",
      "['worst', ['day']]\n",
      "['day', ['worst']]\n",
      "['life', []]\n",
      "Review Number :  34\n",
      "[['wooo', []], ['real', ['edgy']], ['edgy', ['real']], ['ziyuan', []], ['novel', ['writer']], ['writer', ['you', 'novel']]]\n",
      "['wooo', []]\n",
      "['real', ['edgy']]\n",
      "['edgy', ['real']]\n",
      "['ziyuan', []]\n",
      "['novel', ['writer']]\n",
      "['writer', ['you', 'novel']]\n",
      "Review Number :  35\n",
      "[['highly', ['inaccessible']], ['inaccessible', ['highly']]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hard', ['so']], ['get', ['there']], ['mrt', ['stations']], ['stations', ['not', 'mrt']]]\n",
      "['highly', ['inaccessible']]\n",
      "['inaccessible', ['highly']]\n",
      "['hard', ['so']]\n",
      "['get', ['there']]\n",
      "['mrt', ['stations']]\n",
      "['stations', ['not', 'mrt']]\n",
      "Review Number :  36\n",
      "[['bane', []], ['existence', []]]\n",
      "['bane', []]\n",
      "['existence', []]\n",
      "Review Number :  37\n",
      "[['cookhouse', []], ['book', ['get', 'everyday']], ['everyday', ['book']]]\n",
      "[['troublesome', ['most', 'thing']], ['thing', ['troublesome', 'discussing']], ['discussing', ['thing', 'what']], ['lunch', []]]\n",
      "[['sidenote', []], ['菜贩', ['canteenb']], ['@', []], ['canteen', []], ['canteenb', ['菜贩', '@canteen', 'bad']], ['bad', ['canteenb', 'real']], ['real', ['bad']], ['bad', ['canteenb', 'real']]]\n",
      "['cookhouse', []]\n",
      "['book', ['get', 'everyday']]\n",
      "['everyday', ['book']]\n",
      "['troublesome', ['most', 'thing']]\n",
      "['thing', ['troublesome', 'discussing']]\n",
      "['discussing', ['thing', 'what']]\n",
      "['lunch', []]\n",
      "['sidenote', []]\n",
      "['菜贩', ['canteenb']]\n",
      "['@', []]\n",
      "['canteen', []]\n",
      "['canteenb', ['菜贩', '@canteen', 'bad']]\n",
      "['bad', ['canteenb', 'real']]\n",
      "['real', ['bad']]\n",
      "['bad', ['canteenb', 'real']]\n",
      "Review Number :  38\n",
      "[['people', ['go']], ['even', ['go']], ['place', []], ['middle', []], ['nowhere', []]]\n",
      "['people', ['go']]\n",
      "['even', ['go']]\n",
      "['place', []]\n",
      "['middle', []]\n",
      "['nowhere', []]\n",
      "Review Number :  39\n",
      "[['officertalk', ['prepared']], ['money', ['own']], ['hard', ['very']], ['middle', []], ['village', []], ['something', []]]\n",
      "['officertalk', ['prepared']]\n",
      "['money', ['own']]\n",
      "['hard', ['very']]\n",
      "['middle', []]\n",
      "['village', []]\n",
      "['something', []]\n",
      "Review Number :  40\n",
      "[['rude', ['staff']], ['staff', ['rude']]]\n",
      "['rude', ['staff']]\n",
      "['staff', ['rude']]\n",
      "Review Number :  41\n",
      "[['inaccessible', ['need']], ['travel', ['time']], ['time', ['1h', 'travel']], ['waste', ['more']], ['time', ['1h', 'travel']]]\n",
      "['inaccessible', ['need']]\n",
      "['travel', ['time']]\n",
      "['time', ['1h', 'travel']]\n",
      "['waste', ['more']]\n",
      "['time', ['1h', 'travel']]\n",
      "Review Number :  42\n",
      "[['far', ['so', 'away']], ['away', ['far']], ['middle', []], ['nowhere', []]]\n",
      "['far', ['so', 'away']]\n",
      "['away', ['far']]\n",
      "['middle', []]\n",
      "['nowhere', []]\n",
      "Review Number :  43\n",
      "[['bad', ['very', 'troopers', 'very', 'attitude']], ['security', ['troopers']], ['troopers', ['bad', 'security', 'have']], ['bad', ['very', 'troopers', 'very', 'attitude']], ['attitude', ['bad', 'have']], ['towards', []], ['public', []]]\n",
      "['bad', ['very', 'troopers', 'very', 'attitude']]\n",
      "['security', ['troopers']]\n",
      "['troopers', ['bad', 'security', 'have']]\n",
      "['bad', ['very', 'troopers', 'very', 'attitude']]\n",
      "['attitude', ['bad', 'have']]\n",
      "['towards', []]\n",
      "['public', []]\n",
      "Review Number :  44\n",
      "[['middle', []], ['nowhere', []]]\n",
      "['middle', []]\n",
      "['nowhere', []]\n",
      "Review Number :  45\n",
      "[['place', []], ['well', ['kept']], ['people', ['rude']], ['unbelievably', ['rude']], ['rude', ['people', 'unbelievably']]]\n",
      "['place', []]\n",
      "['well', ['kept']]\n",
      "['people', ['rude']]\n",
      "['unbelievably', ['rude']]\n",
      "['rude', ['people', 'unbelievably']]\n",
      "Review Number :  46\n",
      "[['bad', ['service']], ['service', ['bad']]]\n",
      "['bad', ['service']]\n",
      "['service', ['bad']]\n",
      "Review Number :  47\n",
      "[['interestingly', ['enough']], ['enough', ['interestingly', 'removed']], ['negative', ['reviews']], ['reviews', ['negative']]]\n",
      "['interestingly', ['enough']]\n",
      "['enough', ['interestingly', 'removed']]\n",
      "['negative', ['reviews']]\n",
      "['reviews', ['negative']]\n",
      "Review Number :  48\n",
      "[['bad', ['service']], ['service', ['bad']]]\n",
      "['bad', ['service']]\n",
      "['service', ['bad']]\n",
      "Review Number :  49\n",
      "[['sheat', []]]\n",
      "[['dirty', ['pigs']], ['pigs', ['dirty', 'training']], ['step', []], ['minefields', []]]\n",
      "['sheat', []]\n",
      "['dirty', ['pigs']]\n",
      "['pigs', ['dirty', 'training']]\n",
      "['step', []]\n",
      "['minefields', []]\n",
      "Review Number :  50\n",
      "[['gncpresent', []]]\n",
      "['gncpresent', []]\n",
      "Review Number :  51\n"
     ]
    }
   ],
   "source": [
    "rdr = pd.read_csv('../../GReviewsData/cmpb.csv')\n",
    "\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "a, b = do_extraction(rdr, nlp)\n",
    "final_sent = get_sentiment(a, b, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg_sent</th>\n",
       "      <th>Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>staff</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical</th>\n",
       "      <td>1.200000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>1.250000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>1.058824</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n't</th>\n",
       "      <td>1.300000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>directions</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expect</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whole</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>process</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pigs</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Avg_sent  Freq\n",
       "staff       0.909091    11\n",
       "medical     1.200000    10\n",
       "people      1.250000    10\n",
       "bad         1.058824     8\n",
       "n't         1.300000     8\n",
       "...              ...   ...\n",
       "directions  0.000000     1\n",
       "expect      1.000000     1\n",
       "whole       1.000000     1\n",
       "process     1.000000     1\n",
       "pigs        1.000000     1\n",
       "\n",
       "[189 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
