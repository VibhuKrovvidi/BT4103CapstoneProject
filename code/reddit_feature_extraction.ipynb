{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "posts = pd.read_csv(\"../ScrapedOutput/nationalservicesg_posts.csv\")\n",
    "comments = pd.read_csv(\"../ScrapedOutput/nationalservicesg_comments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tzemin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\generic.py:5168: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Too many ailments? Should I RSI or 'appease' m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>upcoming re BMT FFI question                  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anyone here has visited S1 branch in Pasir Len...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PES for severely underweight                  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PES E1, but I can't swim.                     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Countdown time to ord period                  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Anyone here went to SPF OCT?                  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NA Exit Permit/ Deferment                     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Recruits having their first water parade      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>RE BMT FFI coming up bruh                     ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Content\n",
       "0   Too many ailments? Should I RSI or 'appease' m...\n",
       "1   upcoming re BMT FFI question                  ...\n",
       "2   Anyone here has visited S1 branch in Pasir Len...\n",
       "3   PES for severely underweight                  ...\n",
       "4   PES E1, but I can't swim.                     ...\n",
       "..                                                ...\n",
       "95  Countdown time to ord period                  ...\n",
       "96  Anyone here went to SPF OCT?                  ...\n",
       "97  NA Exit Permit/ Deferment                     ...\n",
       "98  Recruits having their first water parade      ...\n",
       "99  RE BMT FFI coming up bruh                     ...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the two datasets into one for feature extraction\n",
    "posts[posts.body == \"[removed]\"].body = \"\"\n",
    "posts[posts.body == \"[deleted]\"].body = \"\"\n",
    "posts[\"title1\"] = posts[\"title\"].str.ljust(len(posts[\"title\"]) + 1, \" \")\n",
    "posts[\"Content\"] = posts[\"title1\"] + posts[\"body\"]\n",
    "posts = posts.filter(items = [\"Content\"])\n",
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just keep on going to the mo. Take it from me,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All legit means just go and RSI ba. Unless u w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I mean, if you’re really worried about a bad i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>after reading ur post, I think u need to file ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's not your fault that your superiors are ig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>Incredible, I’m at 173, please teach me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>You can call yhe NS hotline and ask. But most ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>[https://www.cmpb.gov.sg/web/portal/cmpb/home/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>You can opt to come back every 3 months I thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>Take MC on ffi day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>804 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Content\n",
       "0    Just keep on going to the mo. Take it from me,...\n",
       "1    All legit means just go and RSI ba. Unless u w...\n",
       "2    I mean, if you’re really worried about a bad i...\n",
       "3    after reading ur post, I think u need to file ...\n",
       "4    It's not your fault that your superiors are ig...\n",
       "..                                                 ...\n",
       "799            Incredible, I’m at 173, please teach me\n",
       "800  You can call yhe NS hotline and ask. But most ...\n",
       "801  [https://www.cmpb.gov.sg/web/portal/cmpb/home/...\n",
       "802  You can opt to come back every 3 months I thin...\n",
       "803                                 Take MC on ffi day\n",
       "\n",
       "[804 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments[comments.body == \"[removed]\"].body = \"\"\n",
    "comments = comments.filter(items = [\"body\"]).rename(columns = {\"body\": \"Content\"})\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Too many ailments? Should I RSI or 'appease' m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>upcoming re BMT FFI question                  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anyone here has visited S1 branch in Pasir Len...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PES for severely underweight                  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PES E1, but I can't swim.                     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>Incredible, I’m at 173, please teach me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>You can call yhe NS hotline and ask. But most ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>[https://www.cmpb.gov.sg/web/portal/cmpb/home/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>You can opt to come back every 3 months I thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>Take MC on ffi day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>904 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Content\n",
       "0    Too many ailments? Should I RSI or 'appease' m...\n",
       "1    upcoming re BMT FFI question                  ...\n",
       "2    Anyone here has visited S1 branch in Pasir Len...\n",
       "3    PES for severely underweight                  ...\n",
       "4    PES E1, but I can't swim.                     ...\n",
       "..                                                 ...\n",
       "899            Incredible, I’m at 173, please teach me\n",
       "900  You can call yhe NS hotline and ask. But most ...\n",
       "901  [https://www.cmpb.gov.sg/web/portal/cmpb/home/...\n",
       "902  You can opt to come back every 3 months I thin...\n",
       "903                                 Take MC on ffi day\n",
       "\n",
       "[904 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdata = posts.append(comments, ignore_index = True)\n",
    "textdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction & Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 1.32MB/s]\n",
      "2021-03-07 17:31:49 INFO: Downloading default packages for language: en (English)...\n",
      "2021-03-07 17:31:50 INFO: File exists: C:\\Users\\TzeMin\\stanza_resources\\en\\default.zip.\n",
      "2021-03-07 17:31:54 INFO: Finished downloading models and saved to C:\\Users\\TzeMin\\stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#stanza.download('en') # download English model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(txt, nlp):\n",
    "    try:\n",
    "        txt = txt.lower()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    sentList = nltk.sent_tokenize(txt)\n",
    "\n",
    "    retlist = []\n",
    "    \n",
    "    for line in sentList:\n",
    "        \n",
    "        # Tokenise and POS tag each line\n",
    "        txt_list = nltk.word_tokenize(line)\n",
    "        taggedList = nltk.pos_tag(txt_list)\n",
    "        \n",
    "        # Join noun phrases into a one-word feature\n",
    "        newwordList = []\n",
    "        flag = 0\n",
    "        for i in range(0, len(taggedList)-1):\n",
    "            if(taggedList[i][1] == \"NN\" and taggedList[i+1][1] == \"NN\"):\n",
    "                newwordList.append(taggedList[i][0] + taggedList[i+1][0])\n",
    "                flag = 1\n",
    "            else:\n",
    "                if(flag == 1):\n",
    "                    flag = 0\n",
    "                    continue\n",
    "                newwordList.append(taggedList[i][0])\n",
    "                if(i == len(taggedList)-2):\n",
    "                    newwordList.append(taggedList[i+1][0])\n",
    "        finaltxt = ' '.join(word for word in newwordList)\n",
    "    \n",
    "        # Tokenise and POS tag the new sentence\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        new_txt_list = nltk.word_tokenize(finaltxt)\n",
    "        wordsList = [w for w in new_txt_list if not w in stop_words]\n",
    "        taggedList = nltk.pos_tag(wordsList)\n",
    "        \n",
    "        # Get relations between each word using dependency parsing\n",
    "        doc = nlp(finaltxt)\n",
    "        dep_node = []\n",
    "        try:\n",
    "            for dep_edge in doc.sentences[0].dependencies:\n",
    "                dep_node.append([dep_edge[2].text, dep_edge[0].id, dep_edge[1]])\n",
    "            for i in range(0, len(dep_node)):\n",
    "                if (int(dep_node[i][1]) != 0):\n",
    "                    dep_node[i][1] = newwordList[(int(dep_node[i][1]) - 1)]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Select words corresponding to the below POS\n",
    "        featureList = []\n",
    "        categories = []\n",
    "        for i in taggedList:\n",
    "            if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "                featureList.append(list(i))\n",
    "                categories.append(i[0])        \n",
    "        \n",
    "        fcluster = []\n",
    "        for i in featureList:\n",
    "            filist = []\n",
    "            for j in dep_node:\n",
    "                if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "                    if(j[0]==i[0]):\n",
    "                        filist.append(j[1])\n",
    "                    else:\n",
    "                        filist.append(j[0])\n",
    "            fcluster.append([i[0], filist])\n",
    "        print(fcluster)\n",
    "        \n",
    "        # Remove all features with no sentiment word:\n",
    "        \n",
    "        retlist.append(fcluster)\n",
    "    return retlist;\n",
    "\n",
    "def do_extraction(df, nlp, content_str = \"Content\"):\n",
    "    idx = 0;\n",
    "    review_list = df[content_str].to_list()\n",
    "    feat_count = dict()\n",
    "    feat_sent = dict()\n",
    "    #nlp = stanza.Pipeline('en')\n",
    "    \n",
    "    # Replace \"\" with nan's for removal\n",
    "    df[content_str].replace('', np.nan, inplace=True)\n",
    "    df.dropna(subset=[content_str], inplace=True)\n",
    "    print(\" Processing : \" , df.shape[0], \"rows of data\")\n",
    "    for review in review_list:\n",
    "        print(\"Review Number : \", idx);\n",
    "        idx += 1;\n",
    "        if idx >= df.shape[0]:\n",
    "            break;\n",
    "        try:\n",
    "            output = feature_extraction(review, nlp);\n",
    "        except:\n",
    "            pass;\n",
    "        for sent in output:\n",
    "            for pair in sent:\n",
    "                print(pair)\n",
    "                if pair[0] in feat_sent:\n",
    "                    if pair[1] is not None:\n",
    "                        flist = feat_sent[pair[0]]\n",
    "                        if isinstance(pair[1], list):\n",
    "                            for i in pair[1]:\n",
    "                                flist.append(i)\n",
    "                        else:\n",
    "                            flist.append(pair[1])\n",
    "                        feat_sent[pair[0]] = flist;\n",
    "                else:\n",
    "                    if pair[1] is not None:\n",
    "                        flist = pair[1]\n",
    "                    else:\n",
    "                        flist = list()\n",
    "                    feat_sent[pair[0]] = flist;\n",
    "                \n",
    "                if pair[0] in feat_count:\n",
    "                    feat_count[pair[0]] = feat_count[pair[0]] + 1;\n",
    "                else:\n",
    "                    feat_count[pair[0]] = 1\n",
    "\n",
    "    #print(feat_count);\n",
    "    return feat_count, feat_sent;\n",
    "\n",
    "def get_sentiment(a, b, nlp):\n",
    "\n",
    "    sentiment_score = dict()\n",
    "\n",
    "    # Delete features with no descriptors\n",
    "    cob = b.copy()\n",
    "    for feat in cob.keys():\n",
    "        #print(cob[feat])\n",
    "        if cob[feat] == []:\n",
    "            del b[feat]\n",
    "\n",
    "    # Run pre-built sentiment score and take avg of all descriptors\n",
    "    for f in b.keys():\n",
    "        #print(f);\n",
    "        ssum = 0;\n",
    "        for g in b[f]:\n",
    "            try:\n",
    "                doc = nlp(g);\n",
    "\n",
    "                for i in doc.sentences:\n",
    "\n",
    "                        #print(i.sentiment)\n",
    "                        ssum += i.sentiment;\n",
    "            except:\n",
    "                pass;\n",
    "\n",
    "        sentiment_score[f] = ssum / len(b[f])\n",
    "\n",
    "        adf = pd.DataFrame.from_dict(a, orient='index', columns=['Freq'])\n",
    "    adf.sort_values(by=\"Freq\", ascending=False, inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "    avg_sent = pd.DataFrame.from_dict(sentiment_score, orient='index', columns=[\"Avg_sent\"])\n",
    "\n",
    "    final_sent = avg_sent.merge(adf, left_index=True, right_index=True)\n",
    "    final_sent.sort_values(by=\"Freq\", ascending=False, inplace=True)\n",
    "    return final_sent;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-07 18:29:46 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-03-07 18:29:46 INFO: Use device: cpu\n",
      "2021-03-07 18:29:46 INFO: Loading: tokenize\n",
      "2021-03-07 18:29:46 INFO: Loading: pos\n",
      "2021-03-07 18:29:47 INFO: Loading: lemma\n",
      "2021-03-07 18:29:47 INFO: Loading: depparse\n",
      "2021-03-07 18:29:47 INFO: Loading: sentiment\n",
      "2021-03-07 18:29:48 INFO: Loading: ner\n",
      "2021-03-07 18:29:50 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "rdr = textdata #.head(50)\n",
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing :  888 rows of data\n",
      "Review Number :  0\n",
      "[['many', ['too', 'ailments']], ['ailments', ['many']]]\n",
      "[['rsi', ['i']], ['superiors', [\"'\"]]]\n",
      "[['currently', ['ma']], ['periodic', ['ma']], ['leg', ['injury']], ['injury', ['leg']], ['bmt', []]]\n",
      "[['weeks', []], ['ago', ['rsi-ed']], ['rsi-ed', ['ago', 'i']], ['sharp', ['tinglingpain']], ['tinglingpain', ['sharp']], ['elbows', ['diagnosed']], ['nerveimpingement', []], ['referred', ['me']], ['specialist', []]]\n",
      "[['weeks', []], ['lower', ['developing']], ['back', ['ache']], ['ache', ['back', 'developing']], ['well', []], ['record', []], ['legitimate', ['these']]]\n",
      "[['mo', []], ['specialists', []], ['back', ['ache']], ['ache', ['less', 'back']]]\n",
      "[['rsi', ['i', 'again']], [\"'too\", []], ['much', []]]\n",
      "[['superiors', []], ['already', ['seeing']], ['glasses', ['tinted']], ['sickly', []]]\n",
      "['many', ['too', 'ailments']]\n",
      "['ailments', ['many']]\n",
      "['rsi', ['i']]\n",
      "['superiors', [\"'\"]]\n",
      "['currently', ['ma']]\n",
      "['periodic', ['ma']]\n",
      "['leg', ['injury']]\n",
      "['injury', ['leg']]\n",
      "['bmt', []]\n",
      "['weeks', []]\n",
      "['ago', ['rsi-ed']]\n",
      "['rsi-ed', ['ago', 'i']]\n",
      "['sharp', ['tinglingpain']]\n",
      "['tinglingpain', ['sharp']]\n",
      "['elbows', ['diagnosed']]\n",
      "['nerveimpingement', []]\n",
      "['referred', ['me']]\n",
      "['specialist', []]\n",
      "['weeks', []]\n",
      "['lower', ['developing']]\n",
      "['back', ['ache']]\n",
      "['ache', ['back', 'developing']]\n",
      "['well', []]\n",
      "['record', []]\n",
      "['legitimate', ['these']]\n",
      "['mo', []]\n",
      "['specialists', []]\n",
      "['back', ['ache']]\n",
      "['ache', ['less', 'back']]\n",
      "['rsi', ['i', 'again']]\n",
      "[\"'too\", []]\n",
      "['much', []]\n",
      "['superiors', []]\n",
      "['already', ['seeing']]\n",
      "['glasses', ['tinted']]\n",
      "['sickly', []]\n",
      "Review Number :  1\n",
      "[['rebmt', ['ffi']], ['ffi', ['rebmt', 'question']], ['question', ['upcoming', 'ffi']], [']', []]]\n",
      "['rebmt', ['ffi']]\n",
      "['ffi', ['rebmt', 'question']]\n",
      "['question', ['upcoming', 'ffi']]\n",
      "[']', []]\n",
      "Review Number :  2\n",
      "[['anyone', ['visited', 'here']], ['s1', ['branch']], ['branch', ['s1', 'visited']], ['pasir', []], ['lenacamp', []]]\n",
      "[['reach', ['how', 'i', 'them']]]\n",
      "[['[', []], [']', []]]\n",
      "['anyone', ['visited', 'here']]\n",
      "['s1', ['branch']]\n",
      "['branch', ['s1', 'visited']]\n",
      "['pasir', []]\n",
      "['lenacamp', []]\n",
      "['reach', ['how', 'i', 'them']]\n",
      "['[', []]\n",
      "[']', []]\n",
      "Review Number :  3\n",
      "[['pes', ['atm']], ['severely', ['underweight']], ['underweight', ['severely']], ['still', ['waiting']], ['pes', ['atm']], ['atm', ['pes']]]\n",
      "[['pes', ['get']], ['cos', ['get']], ['imdamn', ['underweight']], ['underweight', ['imdamn', 'get']], ['vision', ['shit']], ['shit', ['vision']]]\n",
      "[['fyi', ['got']], ['pes', ['b4']], ['years', []], ['ago', ['got']]]\n",
      "['pes', ['atm']]\n",
      "['severely', ['underweight']]\n",
      "['underweight', ['severely']]\n",
      "['still', ['waiting']]\n",
      "['pes', ['atm']]\n",
      "['atm', ['pes']]\n",
      "['pes', ['get']]\n",
      "['cos', ['get']]\n",
      "['imdamn', ['underweight']]\n",
      "['underweight', ['imdamn', 'get']]\n",
      "['vision', ['shit']]\n",
      "['shit', ['vision']]\n",
      "['fyi', ['got']]\n",
      "['pes', ['b4']]\n",
      "['years', []]\n",
      "['ago', ['got']]\n",
      "Review Number :  4\n",
      "[['pes', ['e1']], ['e1', ['pes']], [\"n't\", ['swim']]]\n",
      "[['[', []], [']', []]]\n",
      "['pes', ['e1']]\n",
      "['e1', ['pes']]\n",
      "[\"n't\", ['swim']]\n",
      "['[', []]\n",
      "[']', []]\n",
      "Review Number :  5\n",
      "[['anyone', ['have']], ['info', ['have']], ['process', ['goes']], ['ic', []]]\n",
      "[['cmpb', []], ['need', ['you', 'ic']], ['ic', ['need']], ['certain', ['events']], ['events', ['certain']]]\n",
      "[['told', []], ['walk', ['it']], ['process', []]]\n",
      "[['anyone', ['knows']], ['long', ['how', 'it']]]\n",
      "['anyone', ['have']]\n",
      "['info', ['have']]\n",
      "['process', ['goes']]\n",
      "['ic', []]\n",
      "['cmpb', []]\n",
      "['need', ['you', 'ic']]\n",
      "['ic', ['need']]\n",
      "['certain', ['events']]\n",
      "['events', ['certain']]\n",
      "['told', []]\n",
      "['walk', ['it']]\n",
      "['process', []]\n",
      "['anyone', ['knows']]\n",
      "['long', ['how', 'it']]\n",
      "Review Number :  6\n",
      "['anyone', ['have']]\n",
      "['info', ['have']]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-9b45bf0695ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdo_extraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfinal_sent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_sentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-2352f42aa792>\u001b[0m in \u001b[0;36mdo_extraction\u001b[1;34m(df, nlp, content_str)\u001b[0m\n\u001b[0;32m    102\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m                                 \u001b[0mflist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                             \u001b[0mflist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a, b = do_extraction(rdr, nlp)\n",
    "final_sent = get_sentiment(a, b, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg_sent</th>\n",
       "      <th>Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pes</th>\n",
       "      <td>1.166667</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>1.150000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>camp</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anyone</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>really</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assaultpack</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocs</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dump-in</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ceilingfan</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>329 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Avg_sent  Freq\n",
       "pes          1.166667    13\n",
       "get          1.150000    12\n",
       "camp         1.000000    10\n",
       "anyone       1.000000     8\n",
       "really       0.666667     6\n",
       "...               ...   ...\n",
       "assaultpack  1.000000     1\n",
       "ocs          1.000000     1\n",
       "dump-in      1.000000     1\n",
       "time         1.000000     1\n",
       "ceilingfan   1.000000     1\n",
       "\n",
       "[329 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
